{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import cPickle\n",
      "import cPickle\n",
      "import cPickle\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Pytorch multi-GPU Faster R-CNN\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Written by Jiasen Lu, Jianwei Yang, based on code from Ross Girshick\n",
    "# --------------------------------------------------------\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import _init_paths\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import argparse\n",
    "import pprint\n",
    "import pdb\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "\n",
    "from roi_data_layer.roidb_VID import combined_roidb_VID\n",
    "from roi_data_layer.roibatchLoader_VID import roibatchLoader_VID\n",
    "from model.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\n",
    "from model.utils.net_utils import weights_normal_init, save_net, load_net, \\\n",
    "      adjust_learning_rate, save_checkpoint, clip_gradient\n",
    "\n",
    "#from model.faster_rcnn.vgg16 import vgg16\n",
    "#from model.faster_rcnn.resnet import resnet\n",
    "from model.faster_rcnn.faster_rcnn import _fasterRCNN\n",
    "from easydict import EasyDict\n",
    "torch.cuda.set_device(0)\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parse input arguments\n",
    "    \"\"\"\n",
    "    args = EasyDict()\n",
    "    args['dataset'] = 'imagenetVID_1_vid'\n",
    "    args['net'] = 'res101'\n",
    "    args['start_epoch'] = 1\n",
    "    args['max_epochs'] = 1\n",
    "    args['disp_interval'] = 1\n",
    "    args['checkpoint_interval'] = 10000\n",
    "    args['save_dir'] = 'models'\n",
    "    args['num_workers'] = 1\n",
    "    args['cuda'] = True\n",
    "    args['large_scale'] = False\n",
    "    args['mGPUs'] = False\n",
    "    args['batch_size'] = 2\n",
    "    args['vid_size'] = 1\n",
    "    args['class_agnostic'] = False\n",
    "    args['optimizer'] = 'sgd'\n",
    "    args['lr'] = 0.001\n",
    "    args['lr_decay_step'] = 5\n",
    "    args['lr_decay_gamma'] = 0.1\n",
    "    args['session'] = 1\n",
    "    args['resume'] = True\n",
    "    args['checksession'] = 1\n",
    "    args['checkepoch'] = 8\n",
    "    args['checkpoint'] = 27452\n",
    "    args['cfg_file'] = 'cfgs/res101_lighthead.yml'\n",
    "    args['no_save'] = True\n",
    "\n",
    "    return args\n",
    "\n",
    "class batchSampler(BatchSampler):\n",
    "    def __init__(self, sampler, batch_size):\n",
    "        if not isinstance(sampler, Sampler):\n",
    "            raise ValueError(\"sampler should be an instance of \"\n",
    "                             \"torch.utils.data.Sampler, but got sampler={}\"\n",
    "                             .format(sampler))\n",
    "        if batch_size <= 0:\n",
    "            raise ValueError(\"batch_size should be a positive integeral value, \"\n",
    "                             \"but got batch_size={}\".format(batch_size))\n",
    "        self.sampler = sampler\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        for idx in self.sampler:\n",
    "            batch.append(idx)  # Difference: batch.append(int(idx))\n",
    "            if len(batch) == self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if len(batch) > 0:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sampler) / self.batch_size\n",
    "\n",
    "\n",
    "class sampler_imagenet_VID(Sampler):\n",
    "    def __init__(self, train_size, lmdb, batch_size, vid_per_cat = 50, sample_gap_upper_bound = 100):\n",
    "        '''\n",
    "        This sampler samples batches from 1 video every time.\n",
    "        :param train_size: the iteration per epoch.\n",
    "        :param lmdb: the input lmdb.\n",
    "        :param batch_size: number of video pairs for training.\n",
    "        :param vid_per_cat: sampled video number for each category. Default 50.\n",
    "        :param sample_gap_upper_bound: sample_gap_upper_bound is the maximum index gap to sample two images.\n",
    "        '''\n",
    "        assert train_size%batch_size==0, 'train_size should be divided by batch_size.'\n",
    "        self._index_gap_upper_bound = sample_gap_upper_bound/lmdb._gap\n",
    "        structured_indexes = lmdb._structured_indexes\n",
    "        counter = 0\n",
    "        samples = []\n",
    "        while counter<train_size:\n",
    "            # First, we sample the videos from each category.\n",
    "            cat_idxs = list(range(30))\n",
    "            sampled_vids_for_each_category = []\n",
    "            for cat_idx in cat_idxs:\n",
    "                vids = structured_indexes[cat_idx]\n",
    "                if len(vids)>0:\n",
    "                    sampled_vids = random.sample(vids, vid_per_cat)\n",
    "                    sampled_vids_for_each_category.append(sampled_vids)\n",
    "                else:\n",
    "                    sampled_vids_for_each_category.append([])\n",
    "            # Next, we generate training sample indexes.\n",
    "            for vid_id in range(vid_per_cat):\n",
    "                cat_idxs = list(range(30))\n",
    "                random.shuffle(cat_idxs)\n",
    "                for cat_idx in cat_idxs:\n",
    "                    vids = sampled_vids_for_each_category[cat_idx]\n",
    "                    if len(vids)>0:\n",
    "                        vid = vids[vid_id]\n",
    "                        for _ in range(batch_size):\n",
    "                            item = random.sample(vid, 2)\n",
    "                            while item[0]-item[1]>self._index_gap_upper_bound or item[1]-item[0]>self._index_gap_upper_bound:\n",
    "                                item = random.sample(vid, 2)\n",
    "                            samples.append(item)\n",
    "                            counter+=1\n",
    "        self.samples = samples[:train_size]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "def create_tensor_holder():\n",
    "    # initilize the tensor holder here.\n",
    "    im_data = torch.FloatTensor(1)\n",
    "    im_info = torch.FloatTensor(1)\n",
    "    num_boxes = torch.LongTensor(1)\n",
    "    gt_boxes = torch.FloatTensor(1)\n",
    "\n",
    "    # ship to cuda\n",
    "    if args.cuda:\n",
    "        im_data = im_data.cuda()\n",
    "        im_info = im_info.cuda()\n",
    "        num_boxes = num_boxes.cuda()\n",
    "        gt_boxes = gt_boxes.cuda()\n",
    "\n",
    "    # make variable\n",
    "    im_data = Variable(im_data)\n",
    "    im_info = Variable(im_info)\n",
    "    num_boxes = Variable(num_boxes)\n",
    "    gt_boxes = Variable(gt_boxes)\n",
    "    return im_data,im_info,num_boxes,gt_boxes\n",
    "\n",
    "def get_CNN_params(model, lr):\n",
    "    params = []\n",
    "    for key, value in dict(model.named_parameters()).items():\n",
    "        if value.requires_grad:\n",
    "            if 'bias' in key:\n",
    "                params += [{'params': [value], 'lr': lr * (cfg.TRAIN.DOUBLE_BIAS + 1),\n",
    "                            'weight_decay': cfg.TRAIN.BIAS_DECAY and cfg.TRAIN.WEIGHT_DECAY or 0}]\n",
    "            else:\n",
    "                params += [{'params': [value], 'lr': lr, 'weight_decay': cfg.TRAIN.WEIGHT_DECAY}]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called with args:\n",
      "{'checksession': 1, 'cfg_file': 'cfgs/res101_lighthead.yml', 'start_epoch': 1, 'dataset': 'imagenetVID_1_vid', 'session': 1, 'max_epochs': 1, 'mGPUs': False, 'disp_interval': 1, 'resume': True, 'checkpoint': 27452, 'lr': 0.001, 'net': 'res101', 'cuda': True, 'optimizer': 'sgd', 'num_workers': 1, 'batch_size': 2, 'checkepoch': 8, 'lr_decay_step': 5, 'vid_size': 1, 'no_save': True, 'large_scale': False, 'lr_decay_gamma': 0.1, 'save_dir': 'models', 'checkpoint_interval': 10000, 'class_agnostic': False}\n",
      "Using config:\n",
      "{'ANCHOR_RATIOS': [0.5, 1, 2],\n",
      " 'ANCHOR_SCALES': [4, 8, 16, 32],\n",
      " 'CROP_RESIZE_WITH_MAX_POOL': False,\n",
      " 'CUDA': False,\n",
      " 'DATA_DIR': '/home/lvye/lvye/VODProj/faster-rcnn.pytorch/data',\n",
      " 'DEDUP_BOXES': 0.0625,\n",
      " 'EPS': 1e-14,\n",
      " 'EXP_DIR': 'res101',\n",
      " 'FEAT_STRIDE': [16],\n",
      " 'GPU_ID': 0,\n",
      " 'MATLAB': 'matlab',\n",
      " 'MAX_NUM_GT_BOXES': 30,\n",
      " 'MOBILENET': {'DEPTH_MULTIPLIER': 1.0,\n",
      "               'FIXED_LAYERS': 5,\n",
      "               'REGU_DEPTH': False,\n",
      "               'WEIGHT_DECAY': 4e-05},\n",
      " 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),\n",
      " 'POOLING_MODE': 'pspool',\n",
      " 'POOLING_SIZE': 7,\n",
      " 'RESNET': {'CORE_CHOICE': {'FASTER_RCNN': 'faster_rcnn',\n",
      "                            'RFCN': 'rfcn',\n",
      "                            'RFCN_LIGHTHEAD': 'rfcn_light_head',\n",
      "                            'USE': 'rfcn_light_head'},\n",
      "            'FIXED_BLOCKS': 1,\n",
      "            'MAX_POOL': False},\n",
      " 'RNG_SEED': 3,\n",
      " 'ROOT_DIR': '/home/lvye/lvye/VODProj/faster-rcnn.pytorch',\n",
      " 'SIAMESE': {'FG_FRACTION': 1.0,\n",
      "             'RPN_BATCH_SIZE': 256,\n",
      "             'RPN_MIN_SIZE': 8,\n",
      "             'RPN_NEGATIVE_OVERLAP': 0.3,\n",
      "             'RPN_NMS_THRESH': 0.7,\n",
      "             'RPN_POSITIVE_OVERLAP': 0.7,\n",
      "             'RPN_POST_NMS_TOP_N': 1,\n",
      "             'RPN_PRE_NMS_TOP_N': 100,\n",
      "             'TEMPLATE_SEL_BATCH_SIZE': 64,\n",
      "             'TEMPLATE_SEL_BG_THRESH_HI': 0.3,\n",
      "             'TEMPLATE_SEL_BG_THRESH_LO': 0.0,\n",
      "             'TEMPLATE_SEL_CLS_THRESH': 0.8,\n",
      "             'TEMPLATE_SEL_FG_THRESH': 0.7,\n",
      "             'TEMPLATE_SZ': 5},\n",
      " 'TEST': {'BBOX_REG': True,\n",
      "          'HAS_RPN': True,\n",
      "          'MAX_SIZE': 1000,\n",
      "          'MODE': 'nms',\n",
      "          'NMS': 0.3,\n",
      "          'PROPOSAL_METHOD': 'gt',\n",
      "          'RPN_MIN_SIZE': 16,\n",
      "          'RPN_NMS_THRESH': 0.7,\n",
      "          'RPN_POST_NMS_TOP_N': 300,\n",
      "          'RPN_PRE_NMS_TOP_N': 6000,\n",
      "          'RPN_TOP_N': 5000,\n",
      "          'SCALES': [600],\n",
      "          'SVM': False},\n",
      " 'TRAIN': {'ASPECT_GROUPING': False,\n",
      "           'BATCH_SIZE': 128,\n",
      "           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],\n",
      "           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],\n",
      "           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],\n",
      "           'BBOX_NORMALIZE_TARGETS': True,\n",
      "           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': True,\n",
      "           'BBOX_REG': True,\n",
      "           'BBOX_THRESH': 0.5,\n",
      "           'BG_THRESH_HI': 0.5,\n",
      "           'BG_THRESH_LO': 0.0,\n",
      "           'BIAS_DECAY': False,\n",
      "           'BN_TRAIN': False,\n",
      "           'DISPLAY': 20,\n",
      "           'DOUBLE_BIAS': False,\n",
      "           'FG_FRACTION': 0.25,\n",
      "           'FG_THRESH': 0.5,\n",
      "           'GAMMA': 0.1,\n",
      "           'HAS_RPN': True,\n",
      "           'IMS_PER_BATCH': 1,\n",
      "           'LEARNING_RATE': 0.001,\n",
      "           'MAX_SIZE': 1000,\n",
      "           'MOMENTUM': 0.9,\n",
      "           'OHEM': False,\n",
      "           'OHEM_BATCH_SIZE': 128,\n",
      "           'OHEM_NMS': 0.7,\n",
      "           'PROPOSAL_METHOD': 'gt',\n",
      "           'RPN_BATCHSIZE': 256,\n",
      "           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],\n",
      "           'RPN_CLOBBER_POSITIVES': False,\n",
      "           'RPN_FG_FRACTION': 0.5,\n",
      "           'RPN_MIN_SIZE': 8,\n",
      "           'RPN_NEGATIVE_OVERLAP': 0.3,\n",
      "           'RPN_NMS_THRESH': 0.7,\n",
      "           'RPN_POSITIVE_OVERLAP': 0.7,\n",
      "           'RPN_POSITIVE_WEIGHT': -1.0,\n",
      "           'RPN_POST_NMS_TOP_N': 2000,\n",
      "           'RPN_PRE_NMS_TOP_N': 12000,\n",
      "           'SCALES': [600],\n",
      "           'SNAPSHOT_ITERS': 5000,\n",
      "           'SNAPSHOT_KEPT': 3,\n",
      "           'SNAPSHOT_PREFIX': 'res101_faster_rcnn',\n",
      "           'STEPSIZE': [30000],\n",
      "           'SUMMARY_INTERVAL': 180,\n",
      "           'TRIM_HEIGHT': 600,\n",
      "           'TRIM_WIDTH': 600,\n",
      "           'TRUNCATED': False,\n",
      "           'USE_ALL_GT': True,\n",
      "           'USE_FLIPPED': True,\n",
      "           'USE_GT': False,\n",
      "           'WEIGHT_DECAY': 0.0001},\n",
      " 'USE_GPU_NMS': True}\n",
      "Loaded dataset `imagenetVID_1_vid_train` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "/home/lvye/lvye/VODProj/faster-rcnn.pytorch/data/cache/imagenetVID_1_vid_train_gt_roidb.pkl\n",
      "imagenetVID_1_vid_train gt roidb loaded from /home/lvye/lvye/VODProj/faster-rcnn.pytorch/data/cache/imagenetVID_1_vid_train_gt_roidb.pkl\n",
      "Image sizes loaded from /home/lvye/lvye/VODProj/faster-rcnn.pytorch/data/cache/imagenetVID_1_vid_train_sizes.pkl\n",
      "672\n",
      "672\n",
      "done\n",
      "672 roidb entries\n",
      "RCNN uses RFCN Light Head core.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lib/model/utils/config.py:419: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  yaml_cfg = edict(yaml.load(f))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights from data/pretrained_model/resnet101_caffe.pth\n",
      "loading checkpoint models/res101/imagenetDETVID/rfcn_light_head_1_8_27452.pth\n",
      "loaded checkpoint models/res101/imagenetDETVID/rfcn_light_head_1_8_27452.pth\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "\n",
    "print('Called with args:')\n",
    "print(args)\n",
    "\n",
    "if args.dataset == \"imagenet\":\n",
    "    args.imdb_name = \"imagenet_train\"\n",
    "    args.imdbval_name = \"imagenet_val\"\n",
    "    args.set_cfgs = ['ANCHOR_SCALES', '[4, 8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]', 'MAX_NUM_GT_BOXES', '30']\n",
    "elif args.dataset == \"imagenet_10_imgs\":\n",
    "    args.imdb_name = \"imagenet_10_imgs_train\"\n",
    "    args.imdbval_name = \"imagenet_10_imgs_val\"\n",
    "    args.set_cfgs = ['ANCHOR_SCALES', '[4, 8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]', 'MAX_NUM_GT_BOXES', '30']\n",
    "elif args.dataset == \"imagenetVID_1_vid\":\n",
    "    args.imdb_name = 'imagenetVID_1_vid_train'\n",
    "    args.imdbval_name = 'imagenetVID_1_vid_val'\n",
    "    args.set_cfgs = ['ANCHOR_SCALES', '[4, 8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]', 'MAX_NUM_GT_BOXES', '30']\n",
    "elif args.dataset == \"imagenetVID\":\n",
    "    args.imdb_name = 'imagenetVID_train'\n",
    "    args.imdbval_name = 'imagenetVID_val'\n",
    "    args.set_cfgs = ['ANCHOR_SCALES', '[4, 8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]', 'MAX_NUM_GT_BOXES', '30']\n",
    "elif args.dataset == 'imagenetDETVID':\n",
    "    args.imdb_name = 'imagenetDETVID_train'\n",
    "    args.imdbval_name = 'imagenetDETVID_val'\n",
    "    args.set_cfgs = ['ANCHOR_SCALES', '[4, 8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]', 'MAX_NUM_GT_BOXES', '30']\n",
    "\n",
    "if args.cfg_file is None:\n",
    "    args.cfg_file = \"cfgs/{}_ls.yml\".format(args.net) if args.large_scale else \"cfgs/{}.yml\".format(args.net)\n",
    "\n",
    "if args.cfg_file is not None:\n",
    "    cfg_from_file(args.cfg_file)\n",
    "if args.set_cfgs is not None:\n",
    "    cfg_from_list(args.set_cfgs)\n",
    "\n",
    "print('Using config:')\n",
    "pprint.pprint(cfg)\n",
    "np.random.seed(cfg.RNG_SEED)\n",
    "\n",
    "#torch.backends.cudnn.benchmark = True\n",
    "if torch.cuda.is_available() and not args.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "# train set\n",
    "# -- Note: Use validation set and disable the flipped to enable faster loading.\n",
    "cfg.TRAIN.USE_FLIPPED = False\n",
    "cfg.USE_GPU_NMS = args.cuda\n",
    "# TODO change combined_roidb.\n",
    "imdb, roidb, ratio_list, ratio_index = combined_roidb_VID(args.imdb_name)\n",
    "\n",
    "print('{:d} roidb entries'.format(len(roidb)))\n",
    "\n",
    "output_dir = args.save_dir + \"/\" + args.net + \"/\" + args.dataset\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# TODO change the dataloader and sampler.\n",
    "train_size = 150\n",
    "my_sampler = sampler_imagenet_VID(\n",
    "    train_size = train_size, \n",
    "    lmdb=imdb, \n",
    "    batch_size=args.batch_size, \n",
    "    vid_per_cat = 1, \n",
    "    sample_gap_upper_bound = 100)\n",
    "my_batch_sampler = batchSampler(sampler = my_sampler, batch_size=args.batch_size)\n",
    "\n",
    "dataset = roibatchLoader_VID(roidb, ratio_list, ratio_index, args.batch_size, imdb.num_classes, training=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_sampler=my_batch_sampler, num_workers=args.num_workers)\n",
    "\n",
    "if args.cuda:\n",
    "    cfg.CUDA = True\n",
    "\n",
    "# initilize the network here.\n",
    "if args.net == 'res101':\n",
    "    RCNN = _fasterRCNN(imdb.classes, 101, pretrained=True, class_agnostic=args.class_agnostic, b_save_mid_convs = True)\n",
    "elif args.net == 'res50':\n",
    "    RCNN = _fasterRCNN(imdb.classes, 50, pretrained=True, class_agnostic=args.class_agnostic, b_save_mid_convs = True)\n",
    "elif args.net == 'res152':\n",
    "    RCNN = _fasterRCNN(imdb.classes, 152, pretrained=True, class_agnostic=args.class_agnostic, b_save_mid_convs = True)\n",
    "else:\n",
    "    print(\"network is not defined\")\n",
    "    pdb.set_trace()\n",
    "\n",
    "RCNN.create_architecture()\n",
    "\n",
    "lr = cfg.TRAIN.LEARNING_RATE\n",
    "lr = args.lr\n",
    "#tr_momentum = cfg.TRAIN.MOMENTUM\n",
    "#tr_momentum = args.momentum\n",
    "\n",
    "params = []\n",
    "params = get_CNN_params(RCNN, lr)\n",
    "\n",
    "if args.optimizer == \"adam\":\n",
    "    lr = lr * 0.1\n",
    "    optimizer = torch.optim.Adam(params)\n",
    "\n",
    "elif args.optimizer == \"sgd\":\n",
    "    optimizer = torch.optim.SGD(params, momentum=cfg.TRAIN.MOMENTUM)\n",
    "\n",
    "if args.cuda:\n",
    "    RCNN.cuda()\n",
    "\n",
    "if args.resume:\n",
    "    load_name_predix = cfg.RESNET.CORE_CHOICE.USE\n",
    "    if cfg.TRAIN.OHEM is True:\n",
    "        load_name_predix = load_name_predix + '_OHEM'\n",
    "    prefix = args.save_dir + \"/\" + args.net + \"/\" +'imagenetDETVID'\n",
    "    load_name = os.path.join(\n",
    "        prefix,\n",
    "        load_name_predix+'_{}_{}_{}.pth'.format(args.checksession, args.checkepoch, args.checkpoint))\n",
    "print(\"loading checkpoint %s\" % (load_name))\n",
    "checkpoint = torch.load(load_name)\n",
    "args.session = checkpoint['session']\n",
    "args.start_epoch = checkpoint['epoch']\n",
    "RCNN.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "lr = optimizer.param_groups[0]['lr']\n",
    "if 'pooling_mode' in checkpoint.keys():\n",
    "    cfg.POOLING_MODE = checkpoint['pooling_mode']\n",
    "print(\"loaded checkpoint %s\" % (load_name))\n",
    "\n",
    "if args.mGPUs:\n",
    "    RCNN = nn.DataParallel(RCNN)\n",
    "\n",
    "iters_per_epoch = int(train_size / args.batch_size)\n",
    "\n",
    "im_data_1, im_info_1, num_boxes_1, gt_boxes_1 = create_tensor_holder()\n",
    "im_data_2, im_info_2, num_boxes_2, gt_boxes_2 = create_tensor_holder()\n",
    "####for epoch in range(args.start_epoch, args.max_epochs + 1):\n",
    "# setting to train mode\n",
    "RCNN.train()\n",
    "loss_temp = 0\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  1], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iter = iter(dataloader)\n",
    "####for step in range(iters_per_epoch):\n",
    "data_1, data_2 = next(data_iter)\n",
    "\n",
    "im_data_1.data.resize_(data_1[0].size()).copy_(data_1[0])\n",
    "im_info_1.data.resize_(data_1[1].size()).copy_(data_1[1])\n",
    "gt_boxes_1.data.resize_(data_1[2].size()).copy_(data_1[2])\n",
    "num_boxes_1.data.resize_(data_1[3].size()).copy_(data_1[3])\n",
    "\n",
    "im_data_2.data.resize_(data_2[0].size()).copy_(data_2[0])\n",
    "im_info_2.data.resize_(data_2[1].size()).copy_(data_2[1])\n",
    "gt_boxes_2.data.resize_(data_2[2].size()).copy_(data_2[2])\n",
    "num_boxes_2.data.resize_(data_2[3].size()).copy_(data_2[3])\n",
    "\n",
    "print(im_data_1.shape)\n",
    "#print(im_data.shape)\n",
    "RCNN.zero_grad()\n",
    "##################################\n",
    "#        Detection part          #\n",
    "##################################\n",
    "# detection loss for image 1.\n",
    "rois_1, cls_prob_1, bbox_pred_1, \\\n",
    "rpn_loss_cls_1, rpn_loss_box_1, \\\n",
    "RCNN_loss_cls_1, RCNN_loss_bbox_1, \\\n",
    "rois_label_1 = RCNN(im_data_1, im_info_1, gt_boxes_1, num_boxes_1)\n",
    "\n",
    "#c3_1, c4_1, c5_1 = RCNN.c_3, RCNN.c_4, RCNN.c_5\n",
    "conv4_feat_1 = RCNN.Conv4_feat\n",
    "rpn_rois_1 = RCNN.rpn_rois\n",
    "\n",
    "loss = rpn_loss_cls_1.mean() + rpn_loss_box_1.mean() \\\n",
    "   + RCNN_loss_cls_1.mean() + RCNN_loss_bbox_1.mean()\n",
    "\n",
    "# detection loss for image 2.\n",
    "rois_2, cls_prob_2, bbox_pred_2, \\\n",
    "rpn_loss_cls_2, rpn_loss_box_2, \\\n",
    "RCNN_loss_cls_2, RCNN_loss_bbox_2, \\\n",
    "rois_label_2 = RCNN(im_data_2, im_info_2, gt_boxes_2, num_boxes_2)\n",
    "\n",
    "#c3_2, c4_2, c5_2 = RCNN.c_3, RCNN.c_4, RCNN.c_5\n",
    "conv4_feat_2 = RCNN.Conv4_feat\n",
    "rpn_rois_2 = RCNN.rpn_rois\n",
    "\n",
    "# beware, need to use += operation here.\n",
    "loss += rpn_loss_cls_2.mean() + rpn_loss_box_2.mean() \\\n",
    "     + RCNN_loss_cls_2.mean() + RCNN_loss_bbox_2.mean()\n",
    "\n",
    "##################################\n",
    "#        Tracking part           #\n",
    "##################################\n",
    "# define tracking loss here.\n",
    "\n",
    "loss_temp += loss.item()\n",
    "\n",
    "'''\n",
    "# backward\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "if args.net == \"vgg16\":\n",
    "    clip_gradient(RCNN, 10.)\n",
    "optimizer.step()\n",
    "\n",
    "if step % args.disp_interval == 0:\n",
    "    end = time.time()\n",
    "    if step > 0:\n",
    "        loss_temp /= (args.disp_interval + 1)\n",
    "\n",
    "    if args.mGPUs:\n",
    "        loss_rpn_cls = rpn_loss_cls.mean().item()\n",
    "        loss_rpn_box = rpn_loss_box.mean().item()\n",
    "        loss_rcnn_cls = RCNN_loss_cls.mean().item()\n",
    "        loss_rcnn_box = RCNN_loss_bbox.mean().item()\n",
    "        fg_cnt = torch.sum(rois_label.data.ne(0))\n",
    "        bg_cnt = rois_label.data.numel() - fg_cnt\n",
    "    else:\n",
    "        loss_rpn_cls = rpn_loss_cls.item()\n",
    "        loss_rpn_box = rpn_loss_box.item()\n",
    "        loss_rcnn_cls = RCNN_loss_cls.item()\n",
    "        loss_rcnn_box = RCNN_loss_bbox.item()\n",
    "        fg_cnt = torch.sum(rois_label.data.ne(0))\n",
    "        bg_cnt = rois_label.data.numel() - fg_cnt\n",
    "\n",
    "    print(\"[session %d][epoch %2d][iter %4d/%4d] loss: %.4f, lr: %.2e\" \\\n",
    "                            % (args.session, epoch, step, iters_per_epoch, loss_temp, lr))\n",
    "    print(\"\\t\\t\\tfg/bg=(%d/%d), time cost: %f\" % (fg_cnt, bg_cnt, end-start))\n",
    "    print(\"\\t\\t\\trpn_cls: %.4f, rpn_box: %.4f, rcnn_cls: %.4f, rcnn_box %.4f\" \\\n",
    "                  % (loss_rpn_cls, loss_rpn_box, loss_rcnn_cls, loss_rcnn_box))\n",
    "\n",
    "    loss_temp = 0\n",
    "    start = time.time()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the data shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1024, 38, 67])\n",
      "torch.Size([2, 2000, 5])\n",
      "torch.Size([2, 1024, 38, 67])\n",
      "torch.Size([2, 2000, 5])\n"
     ]
    }
   ],
   "source": [
    "print(conv4_feat_1.shape)\n",
    "print(rpn_rois_1.shape)\n",
    "print(conv4_feat_2.shape)\n",
    "print(rpn_rois_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test siamese RPN data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from siamese_net.siameseRPN import siameseRPN\n",
    "#from siamese_net.template_proposal_layer import _TemplateProposalLayer\n",
    "#from siamese_net.template_target_proposal_layer import _TemplateTargetProposalLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.SIAMESE.TEMPLATE_SZ = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# propose template images.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from model.utils.config import cfg\n",
    "from model.rpn.bbox_transform import bbox_overlaps_batch, bbox_transform_batch\n",
    "\n",
    "class _TemplateProposalLayer(nn.Module):\n",
    "    '''\n",
    "    Propose template for siamese RPN.\n",
    "    The functionality of this layer for training and testing are different.\n",
    "    For training, it selects training samples from RPN output.\n",
    "    The returned rois and track_ids are used for training sample preparation.\n",
    "    For testing, it selects predicted boxes with high enough score.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(_TemplateProposalLayer, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.training:\n",
    "            all_rois, gt_boxes = inputs\n",
    "            rois, labels, track_ids = self.propose_template_training(all_rois, gt_boxes)\n",
    "            return rois, labels, track_ids\n",
    "        else:\n",
    "            bbox_pred, cls_prob, track_ids = inputs\n",
    "            bbox_pred, cls_prob, track_ids = self.propose_template_testing(bbox_pred, cls_prob, track_ids)\n",
    "            return bbox_pred, cls_prob, track_ids\n",
    "\n",
    "    def backward(self, top, propagate_down, bottom):\n",
    "        \"\"\"This layer does not propagate gradients.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def reshape(self, bottom, top):\n",
    "        \"\"\"Reshaping happens during the call to forward.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def propose_template_testing(self, bbox_pred, cls_prob, track_ids):\n",
    "        sel_ind = torch.where(cls_prob>cfg.SIAMESE.TEMPLATE_SEL_CLS_THRESH)[0]\n",
    "        return bbox_pred[sel_ind], cls_prob[sel_ind], track_ids[sel_ind]\n",
    "\n",
    "    def propose_template_training(self, all_rois, gt_boxes):\n",
    "        gt_boxes_append = gt_boxes.new_zeros((gt_boxes.size()[0], gt_boxes.size()[1], 5))\n",
    "        gt_boxes_append[:, :, 1:5] = gt_boxes[:, :, :4]\n",
    "\n",
    "        # Include ground-truth boxes in the set of candidate rois\n",
    "        all_rois = torch.cat([all_rois, gt_boxes_append], 1)\n",
    "\n",
    "        num_images = 1\n",
    "        rois_per_image = int(cfg.SIAMESE.TEMPLATE_SEL_BATCH_SIZE / num_images)\n",
    "        fg_rois_per_image = int(np.round(cfg.SIAMESE.FG_FRACTION * rois_per_image))\n",
    "        fg_rois_per_image = 1 if fg_rois_per_image == 0 else fg_rois_per_image\n",
    "\n",
    "        rois, labels, track_ids = self.sample_rois_pytorch(all_rois, gt_boxes, fg_rois_per_image, rois_per_image)\n",
    "        return rois, labels, track_ids\n",
    "\n",
    "    def sample_rois_pytorch(self, all_rois, gt_boxes, fg_rois_per_image, rois_per_image):\n",
    "        \"\"\"Generate a random sample of template RoIs comprising foreground and background\n",
    "        examples.\n",
    "        \"\"\"\n",
    "        # overlaps: (rois x gt_boxes)\n",
    "\n",
    "        overlaps = bbox_overlaps_batch(all_rois, gt_boxes)\n",
    "\n",
    "        max_overlaps, gt_assignment = torch.max(overlaps, 2)\n",
    "\n",
    "        batch_size = overlaps.size(0)\n",
    "        num_proposal = overlaps.size(1)\n",
    "        num_boxes_per_img = overlaps.size(2)\n",
    "\n",
    "        offset = torch.arange(0, batch_size) * gt_boxes.size(1)\n",
    "        offset = offset.view(-1, 1).type_as(gt_assignment) + gt_assignment\n",
    "\n",
    "        labels = gt_boxes[:, :, 4].contiguous().view(-1).index((offset.view(-1),)).view(batch_size, -1)\n",
    "        track_id = gt_boxes[:, :, 5].contiguous().view(-1).index((offset.view(-1),)).view(batch_size, -1)\n",
    "\n",
    "        labels_batch = labels.new(batch_size, rois_per_image).zero_()\n",
    "        track_id_batch = track_id.new(batch_size, rois_per_image).zero_()-1\n",
    "\n",
    "        rois_batch = all_rois.new(batch_size, rois_per_image, 5).zero_()\n",
    "\n",
    "        # Guard against the case when an image has fewer than max_fg_rois_per_image\n",
    "        # foreground RoIs\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            fg_inds = torch.nonzero(max_overlaps[i] >= cfg.SIAMESE.TEMPLATE_SEL_FG_THRESH).view(-1)\n",
    "            fg_num_rois = fg_inds.numel()\n",
    "\n",
    "            # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n",
    "            bg_inds = torch.nonzero((max_overlaps[i] < cfg.SIAMESE.TEMPLATE_SEL_BG_THRESH_HI) &\n",
    "                                    (max_overlaps[i] >= cfg.SIAMESE.TEMPLATE_SEL_BG_THRESH_LO)).view(-1)\n",
    "            bg_num_rois = bg_inds.numel()\n",
    "\n",
    "            if fg_num_rois > 0 and bg_num_rois > 0:\n",
    "                # sampling fg\n",
    "                fg_rois_per_this_image = min(fg_rois_per_image, fg_num_rois)\n",
    "\n",
    "                # torch.randperm seems has a bug on multi-gpu setting that cause the segfault.\n",
    "                # See https://github.com/pytorch/pytorch/issues/1868 for more details.\n",
    "                # use numpy instead.\n",
    "                # rand_num = torch.randperm(fg_num_rois).long().cuda()\n",
    "                rand_num = torch.from_numpy(np.random.permutation(fg_num_rois)).type_as(gt_boxes).long()\n",
    "                fg_inds = fg_inds[rand_num[:fg_rois_per_this_image]]\n",
    "\n",
    "                # sampling bg\n",
    "                bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n",
    "\n",
    "                # Seems torch.rand has a bug, it will generate very large number and make an error.\n",
    "                # We use numpy rand instead.\n",
    "                # rand_num = (torch.rand(bg_rois_per_this_image) * bg_num_rois).long().cuda()\n",
    "                rand_num = np.floor(np.random.rand(bg_rois_per_this_image) * bg_num_rois)\n",
    "                rand_num = torch.from_numpy(rand_num).type_as(gt_boxes).long()\n",
    "                bg_inds = bg_inds[rand_num]\n",
    "\n",
    "            elif fg_num_rois > 0 and bg_num_rois == 0:\n",
    "                # sampling fg\n",
    "                # rand_num = torch.floor(torch.rand(rois_per_image) * fg_num_rois).long().cuda()\n",
    "                rand_num = np.floor(np.random.rand(rois_per_image) * fg_num_rois)\n",
    "                rand_num = torch.from_numpy(rand_num).type_as(gt_boxes).long()\n",
    "                fg_inds = fg_inds[rand_num]\n",
    "                fg_rois_per_this_image = rois_per_image\n",
    "                bg_rois_per_this_image = 0\n",
    "            elif bg_num_rois > 0 and fg_num_rois == 0:\n",
    "                # sampling bg\n",
    "                # rand_num = torch.floor(torch.rand(rois_per_image) * bg_num_rois).long().cuda()\n",
    "                rand_num = np.floor(np.random.rand(rois_per_image) * bg_num_rois)\n",
    "                rand_num = torch.from_numpy(rand_num).type_as(gt_boxes).long()\n",
    "\n",
    "                bg_inds = bg_inds[rand_num]\n",
    "                bg_rois_per_this_image = rois_per_image\n",
    "                fg_rois_per_this_image = 0\n",
    "            else:\n",
    "                raise ValueError(\"bg_num_rois = 0 and fg_num_rois = 0, this should not happen!\")\n",
    "\n",
    "            # The indices that we're selecting (both fg and bg)\n",
    "            keep_inds = torch.cat([fg_inds, bg_inds], 0)\n",
    "\n",
    "            # Select sampled values from various arrays:\n",
    "            labels_batch[i].copy_(labels[i][keep_inds])\n",
    "            track_id_batch[i].copy_(track_id[i][keep_inds])\n",
    "\n",
    "            # Clamp labels for the background RoIs to 0\n",
    "            if fg_rois_per_this_image < rois_per_image:\n",
    "                labels_batch[i][fg_rois_per_this_image:] = 0\n",
    "                track_id_batch[i][fg_rois_per_this_image:] = -1\n",
    "\n",
    "            rois_batch[i] = all_rois[i][keep_inds]\n",
    "            rois_batch[i, :, 0] = i\n",
    "\n",
    "        return rois_batch, labels_batch, track_id_batch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from model.utils.config import cfg\n",
    "from model.roi_align.modules.roi_align import RoIAlignAvg\n",
    "\n",
    "class _TemplateTargetProposalLayer(nn.Module):\n",
    "    '''\n",
    "    prepare template and target training pairs.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(_TemplateTargetProposalLayer,self).__init__()\n",
    "        self.template_proposal_layer = _TemplateProposalLayer()\n",
    "        self.RCNN_roi_align = RoIAlignAvg(cfg.SIAMESE.TEMPLATE_SZ, cfg.SIAMESE.TEMPLATE_SZ, 1.0 / 16.0)\n",
    "        # TODO change back\n",
    "        #self.RCNN_roi_align = RoIAlignAvg(cfg.SIAMESE.TEMPLATE_SZ, cfg.SIAMESE.TEMPLATE_SZ, 1.0)\n",
    "\n",
    "    def forward(self, feats1, feats2, rpn_rois_1, gt_boxes_1, gt_boxes_2):\n",
    "        '''\n",
    "\n",
    "        :param feats1: size (N,C,H,W)\n",
    "        :param feats2: size (N,C,H,W)\n",
    "        :param rpn_rois_1: size (N,n,5) default n==256\n",
    "        :param rpn_rois_2: size (N,n,5)\n",
    "        :param gt_boxes_1: size (N,n,6) default n==128\n",
    "        :param gt_boxes_2: size (N,n,6)\n",
    "        :return:\n",
    "        '''\n",
    "        # feats 1 is template source, feats2 is target source.\n",
    "        batch_size = feats1.size(0)\n",
    "        # template_rois size (N,n,5) N:number of batches. n:number of rois.\n",
    "        # template_labels size (N,n)\n",
    "        # template_track_ids size (N,n)\n",
    "        template_rois_all, template_labels_all, template_track_ids_all = self.template_proposal_layer((rpn_rois_1, gt_boxes_1))\n",
    "        template_weights_all = self.crop_weights_from_feats(feats1, template_rois_all).view(\n",
    "            batch_size,\n",
    "            template_rois_all.size(1),\n",
    "            feats1.size(1),\n",
    "            cfg.SIAMESE.TEMPLATE_SZ,\n",
    "            cfg.SIAMESE.TEMPLATE_SZ)\n",
    "\n",
    "        # for each item, it is (target_feat, template_weights, gt_boxes for each weight).\n",
    "        # target gt_boxes should be of shape (n, 1, 6).\n",
    "        rtv_training_tuples = []\n",
    "        for idx in range(batch_size):\n",
    "            nonzero_coords = torch.nonzero(template_labels_all[idx]>0)\n",
    "            fg_obj_inds = None\n",
    "            if nonzero_coords.size(0)>0:\n",
    "                fg_obj_inds = nonzero_coords[:,0] # extracting rows.\n",
    "            else:\n",
    "                continue\n",
    "            target_feat = feats2[idx:idx+1]\n",
    "            template_weights = template_weights_all[idx]\n",
    "            template_track_ids = template_track_ids_all[idx]            \n",
    "            target_gt_boxes_all = gt_boxes_2[idx,:,:]\n",
    "            target_gt_boxes = []\n",
    "            \n",
    "            for template_id in range(template_weights.size(0)):\n",
    "                template_track_id = template_track_ids[template_id]\n",
    "                has_gt = False\n",
    "                if template_track_id>=0:\n",
    "                    for gt_box_2 in target_gt_boxes_all:\n",
    "                        if gt_box_2[5]==template_track_id:\n",
    "                            has_gt=True\n",
    "                            target_gt_boxes.append(gt_box_2.view(1,-1))\n",
    "                            break\n",
    "                if not has_gt:\n",
    "                    target_gt_boxes.append(gt_box_2.new_zeros(1, 6))\n",
    "                    \n",
    "            target_gt_boxes = torch.stack(target_gt_boxes)\n",
    "            template_weights = template_weights.index_select(0,fg_obj_inds)\n",
    "            target_gt_boxes = target_gt_boxes.index_select(0,fg_obj_inds)\n",
    "            \n",
    "            rtv_training_tuples.append((target_feat, template_weights, target_gt_boxes))\n",
    "\n",
    "        return rtv_training_tuples\n",
    "\n",
    "    def backward(self, top, propagate_down, bottom):\n",
    "        \"\"\"This layer does not propagate gradients.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def reshape(self, bottom, top):\n",
    "        \"\"\"Reshaping happens during the call to forward.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def crop_weights_from_feats(self, feats, rois):\n",
    "        return self.RCNN_roi_align(feats, rois.view(-1, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from model.utils.config import cfg\n",
    "from model.rpn.proposal_layer import _ProposalLayer\n",
    "from model.rpn.anchor_target_layer import _AnchorTargetLayer\n",
    "from model.utils.net_utils import _smooth_l1_loss\n",
    "\n",
    "class siameseRPN(nn.Module):\n",
    "    def __init__(self, input_dim, anchor_scales, anchor_ratios, use_separable_correlation = False):\n",
    "        super(siameseRPN, self).__init__()\n",
    "        self.use_separable_correlation = use_separable_correlation\n",
    "        self.din = input_dim  # get depth of input feature map, e.g., 512\n",
    "        self.correlation_channel = 256\n",
    "        self.anchor_scales = anchor_scales\n",
    "        self.anchor_ratios = anchor_ratios\n",
    "\n",
    "        # TODO this may be modified if used for other strides.\n",
    "        self.feat_stride = cfg.FEAT_STRIDE[0]\n",
    "\n",
    "        # target branch.\n",
    "        self.RPN_Conv_bbox = nn.Conv2d(self.din, self.correlation_channel, 3, 1, 1, bias=True)\n",
    "        self.RPN_Conv_cls = nn.Conv2d(self.din, self.correlation_channel, 3, 1, 1, bias=True)\n",
    "\n",
    "        # template branch.\n",
    "        self.nc_score_out = len(self.anchor_scales) * len(self.anchor_ratios) * 2  # 2(bg/fg) * 9 (anchors)\n",
    "        self.RPN_cls_score = nn.Conv2d(self.din, self.correlation_channel*self.nc_score_out, 1, 1, 0)\n",
    "        # define anchor box offset prediction layer\n",
    "        self.nc_bbox_out = len(self.anchor_scales) * len(self.anchor_ratios) * 4  # 4(coords) * 9 (anchors)\n",
    "        self.RPN_bbox_pred = nn.Conv2d(self.din, self.correlation_channel*self.nc_bbox_out, 1, 1, 0)\n",
    "\n",
    "        # define proposal layer\n",
    "        self.RPN_proposal = _ProposalLayer(self.feat_stride, self.anchor_scales, self.anchor_ratios)\n",
    "\n",
    "        # define anchor target layer\n",
    "        self.RPN_anchor_target = _AnchorTargetLayer(self.feat_stride, self.anchor_scales, self.anchor_ratios)\n",
    "\n",
    "        self.rpn_loss_cls = 0\n",
    "        self.rpn_loss_box = 0\n",
    "\n",
    "\n",
    "        # conv_merge_1x1 is to merge separable convolution to achieve depth-wise separable convolution.\n",
    "        self.conv_merge_1x1 = None\n",
    "        if self.use_separable_correlation:\n",
    "            self.conv_merge_1x1 = nn.Conv2d(self.din, 1, bias=False)\n",
    "        else:\n",
    "            self.bias_cls = nn.Parameter(torch.zeros(self.nc_score_out, 1, 1), requires_grad=True)\n",
    "            self.bias_bbox = nn.Parameter(torch.zeros(self.nc_bbox_out, 1, 1), requires_grad=True)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        def normal_init(m, mean, stddev, truncated=False):\n",
    "            \"\"\"\n",
    "            weight initalizer: truncated normal and random normal.\n",
    "            \"\"\"\n",
    "            # x is a parameter\n",
    "            if truncated:\n",
    "                m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean) # not a perfect approximation\n",
    "            else:\n",
    "                m.weight.data.normal_(mean, stddev)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        normal_init(self.RPN_Conv_cls, 0, 0.001, cfg.TRAIN.TRUNCATED)\n",
    "        normal_init(self.RPN_Conv_bbox, 0, 0.001, cfg.TRAIN.TRUNCATED)\n",
    "        normal_init(self.RPN_cls_score, 0, 0.01, cfg.TRAIN.TRUNCATED)\n",
    "        normal_init(self.RPN_bbox_pred, 0, 0.01, cfg.TRAIN.TRUNCATED)\n",
    "        if self.conv_merge_1x1 is not None:\n",
    "            normal_init(self.conv_merge_1x1, 0, 0.01, cfg.TRAIN.TRUNCATED)\n",
    "            \n",
    "    @staticmethod\n",
    "    def reshape(x, d):\n",
    "        input_shape = x.size()\n",
    "        x = x.view(\n",
    "            input_shape[0],\n",
    "            int(d),\n",
    "            int(float(input_shape[1] * input_shape[2]) / float(d)),\n",
    "            input_shape[3]\n",
    "        )\n",
    "        return x\n",
    "\n",
    "    def cross_correlation(self, target_feat, template_feat, bias=None):\n",
    "        '''\n",
    "        Calculate cross correlation.\n",
    "        :param target_feat: input feature map. It is of size (1,c,H,W)\n",
    "        :param template_feat: convolution kernel. It is of size (N , K, C, kH, kW).\n",
    "                    N is the number of templates. K is the number of anchors. C is the number of correlation channels.\n",
    "        :return: correlation map.\n",
    "        '''\n",
    "        n_templates = template_feat.size(0) #N\n",
    "        out_dim_w = template_feat.size(1) #K\n",
    "        in_dim_w = template_feat.size(2) #C\n",
    "        kh = template_feat.size(3)\n",
    "        kw = template_feat.size(4)\n",
    "        template_feat = template_feat.view(n_templates * out_dim_w, in_dim_w, kh, kw)\n",
    "        H = target_feat.size(2)\n",
    "        W = target_feat.size(3)\n",
    "        \n",
    "        out = nn.functional.conv2d(\n",
    "            target_feat,\n",
    "            template_feat,\n",
    "            bias=None,\n",
    "            stride=1,\n",
    "            padding=int((template_feat.size(2) - 1) / 2),\n",
    "            dilation=1,\n",
    "            groups=1)\n",
    "\n",
    "        out = out.view(n_templates, out_dim_w, H, W)\n",
    "        if bias is not None:\n",
    "            out = out + bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def depth_wise_cross_correlation(self, target_feat, template_feat, bias=None):\n",
    "        '''\n",
    "        Calculate depth-wise cross correlation.\n",
    "        :param target_feat: input feature map. It is of size (1,c,H,W)\n",
    "        :param template_feat: convolution kernel. It is of size (N * out_c, in_c, kH, kW).\n",
    "                    N is the number of templates.\n",
    "        :return: correlation map.\n",
    "        '''\n",
    "        n_templates = template_feat.size(0) #N\n",
    "        out_dim_w = template_feat.size(1) #K\n",
    "        in_dim_w = template_feat.size(2) #C\n",
    "        kh = template_feat.size(3)\n",
    "        kw = template_feat.size(4)\n",
    "        template_feat = template_feat.view(n_templates * out_dim_w * in_dim_w, 1, kh, kw)\n",
    "        H = target_feat.size(2)\n",
    "        W = target_feat.size(3)\n",
    "\n",
    "        out = nn.functional.conv2d(\n",
    "            target_feat,\n",
    "            template_feat,\n",
    "            bias=None,\n",
    "            stride=1,\n",
    "            padding=int((template_feat.size(2)-1)/2),\n",
    "            dilation=1,\n",
    "            groups=in_dim_w)\n",
    "\n",
    "        out = out.view(n_templates*out_dim_w, in_dim_w, H, W)\n",
    "        out = self.conv_merge_1x1(out)\n",
    "        out = out.view(n_templates, out_dim_w, H, W)\n",
    "        if bias is not None:\n",
    "            out = out + bias\n",
    "        return out\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        The inputs are two tuples. One for each image.\n",
    "        :param input holds data (target_feat, im_info, template_feat, gt_boxes, num_boxes)\n",
    "                target_feat is of size (1, C, H, W)\n",
    "                gt_boxes is a batch of gt_boxes for tracking, and is of size (N, 1, 6). 6 represents: x1,y1,x2,y2,class,trackid.\n",
    "                template_feat is of size (N, C, kH, kW).\n",
    "        :return:\n",
    "        '''\n",
    "        if self.training:\n",
    "            target_feat, im_info, template_feat, gt_boxes, num_boxes = input\n",
    "            gt_boxes = gt_boxes[:,:,:5]\n",
    "        else:\n",
    "            target_feat, im_info, template_feat = input\n",
    "\n",
    "        n_templates = template_feat.size(0)\n",
    "        nC = template_feat.size(1)\n",
    "        kh = template_feat.size(2)\n",
    "        kw = template_feat.size(3)\n",
    "        assert self.din == nC, 'The feature dims are not compatible.'\n",
    "        assert nC == target_feat.size(1), 'The feature dims of template_feat and target_feat should be same.'\n",
    "        assert target_feat.size(0) == 1, 'Input target_feat should have a batch size of 1.'\n",
    "\n",
    "        # target branch.\n",
    "        target_feat_cls = self.RPN_Conv_cls(target_feat)\n",
    "        target_feat_bbox = self.RPN_Conv_bbox(target_feat)\n",
    "\n",
    "        # template branch.\n",
    "        template_feat_cls = self.RPN_cls_score(template_feat)\n",
    "        template_feat_bbox = self.RPN_bbox_pred(template_feat)\n",
    "\n",
    "        template_feat_cls = template_feat_cls.view(n_templates, self.nc_score_out, -1, template_feat_cls.size(2), template_feat_cls.size(3))\n",
    "        template_feat_bbox = template_feat_bbox.view(n_templates, self.nc_bbox_out, -1, template_feat_bbox.size(2), template_feat_bbox.size(3))\n",
    "        \n",
    "        # correlation\n",
    "        if self.use_separable_correlation:\n",
    "            rpn_cls_score = self.depth_wise_cross_correlation(target_feat_cls, template_feat_cls, self.bias_cls)\n",
    "            rpn_bbox_pred = self.depth_wise_cross_correlation(target_feat_bbox, template_feat_bbox, self.bias_bbox)\n",
    "        else:\n",
    "            rpn_cls_score = self.cross_correlation(target_feat_cls, template_feat_cls, self.bias_cls)\n",
    "            rpn_bbox_pred = self.cross_correlation(target_feat_bbox, template_feat_bbox, self.bias_bbox)\n",
    "\n",
    "        rpn_cls_score_reshape = self.reshape(rpn_cls_score, 2)\n",
    "        rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape, 1)\n",
    "        rpn_cls_prob = self.reshape(rpn_cls_prob_reshape, self.nc_score_out)\n",
    "\n",
    "        # proposal layer\n",
    "        cfg_key = 'TRAIN' if self.training else 'TEST'\n",
    "\n",
    "        im_info = im_info.expand((rpn_cls_prob.size(0),im_info.size(1)))\n",
    "        rois = self.RPN_proposal((rpn_cls_prob.data, rpn_bbox_pred.data,\n",
    "                                  im_info, cfg_key))\n",
    "\n",
    "        self.rpn_loss_cls = 0\n",
    "        self.rpn_loss_box = 0\n",
    "\n",
    "        # generating training labels and build the rpn loss\n",
    "        if self.training:\n",
    "            assert gt_boxes is not None\n",
    "\n",
    "            batch_size = n_templates\n",
    "            rpn_data = self.RPN_anchor_target((rpn_cls_score.data, gt_boxes, im_info, num_boxes))\n",
    "\n",
    "            # compute classification loss\n",
    "            rpn_cls_score = rpn_cls_score_reshape.permute(0, 2, 3, 1).contiguous().view(batch_size, -1, 2)\n",
    "            rpn_label = rpn_data[0].view(batch_size, -1)\n",
    "\n",
    "            rpn_keep = Variable(rpn_label.view(-1).ne(-1).nonzero().view(-1))\n",
    "            rpn_cls_score = torch.index_select(rpn_cls_score.view(-1, 2), 0, rpn_keep)\n",
    "            rpn_label = torch.index_select(rpn_label.view(-1), 0, rpn_keep.data)\n",
    "            rpn_label = Variable(rpn_label.long())\n",
    "            self.rpn_loss_cls = F.cross_entropy(rpn_cls_score, rpn_label)\n",
    "            fg_cnt = torch.sum(rpn_label.data.ne(0))\n",
    "\n",
    "            rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights = rpn_data[1:]\n",
    "\n",
    "            # compute bbox regression loss\n",
    "            rpn_bbox_inside_weights = Variable(rpn_bbox_inside_weights)\n",
    "            rpn_bbox_outside_weights = Variable(rpn_bbox_outside_weights)\n",
    "            rpn_bbox_targets = Variable(rpn_bbox_targets)\n",
    "\n",
    "            self.rpn_loss_box = _smooth_l1_loss(rpn_bbox_pred, rpn_bbox_targets, rpn_bbox_inside_weights,\n",
    "                                                rpn_bbox_outside_weights, sigma=3, dim=[1, 2, 3])\n",
    "        return rois, self.rpn_loss_cls, self.rpn_loss_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define layer\n",
    "t_t_prop_layer = _TemplateTargetProposalLayer()\n",
    "siameseRPN_layer = siameseRPN(\n",
    "    input_dim = 1024,\n",
    "    anchor_scales = cfg.ANCHOR_SCALES,\n",
    "    anchor_ratios = cfg.ANCHOR_RATIOS,\n",
    "    use_separable_correlation = False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im_info.shape: torch.Size([1, 3])\n",
      "im_info.shape: torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "rtv_training_tuples = t_t_prop_layer(conv4_feat_1, conv4_feat_2, rpn_rois_1, gt_boxes_1, gt_boxes_2)\n",
    "target_feat, template_weights, target_gt_boxes = rtv_training_tuples[0]\n",
    "input_v = (target_feat, \n",
    "           im_info_2[:1], \n",
    "           template_weights,\n",
    "           target_gt_boxes, \n",
    "           1)\n",
    "rois, rpn_loss_cls, rpn_loss_box = siameseRPN_layer(input_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2000, 5])\n",
      "tensor(239.8059, device='cuda:0')\n",
      "tensor(40.7389, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(rois.shape)\n",
    "print(rpn_loss_cls)\n",
    "print(rpn_loss_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_t_prop_layer = _TemplateTargetProposalLayer()\n",
    "siameseRPN_layer = siameseRPN(\n",
    "    input_dim = 1024,\n",
    "    anchor_scales = cfg.ANCHOR_SCALES,\n",
    "    anchor_ratios = cfg.ANCHOR_RATIOS,\n",
    "    use_separable_correlation = False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter    0/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(108/404), time cost: 3.619133\n",
      "\t\t\tsiam_rpn_cls: 124.5178, siam_rpn_box: 34.0088\n",
      "\t\t\trpn_cls: 0.0000, rpn_box: 0.0021, rcnn_cls: 0.0125, rcnn_box 0.0187\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter    1/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(119/393), time cost: 0.990265\n",
      "\t\t\tsiam_rpn_cls: 1.0241, siam_rpn_box: 47.6770\n",
      "\t\t\trpn_cls: 0.0005, rpn_box: 0.0080, rcnn_cls: 0.1088, rcnn_box 0.0505\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter    2/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(67/445), time cost: 0.903464\n",
      "\t\t\tsiam_rpn_cls: 2.3720, siam_rpn_box: 99.1277\n",
      "\t\t\trpn_cls: 0.0742, rpn_box: 0.0300, rcnn_cls: 0.4353, rcnn_box 0.1828\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter    3/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(41/471), time cost: 0.889825\n",
      "\t\t\tsiam_rpn_cls: 113.9259, siam_rpn_box: 25.5296\n",
      "\t\t\trpn_cls: 0.3585, rpn_box: 0.0462, rcnn_cls: 1.4925, rcnn_box 0.3525\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter    4/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(32/480), time cost: 0.862848\n",
      "\t\t\tsiam_rpn_cls: 61.8733, siam_rpn_box: 74.2010\n",
      "\t\t\trpn_cls: 0.3929, rpn_box: 0.0097, rcnn_cls: 1.1649, rcnn_box 0.1666\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter    5/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(17/495), time cost: 0.878251\n",
      "\t\t\tsiam_rpn_cls: 2765.0466, siam_rpn_box: 10705.9961\n",
      "\t\t\trpn_cls: 3.9568, rpn_box: 0.2399, rcnn_cls: 2.2012, rcnn_box 0.1398\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter    6/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(4/508), time cost: 0.805373\n",
      "\t\t\tsiam_rpn_cls: nan, siam_rpn_box: nan\n",
      "\t\t\trpn_cls: 57593148675188478546735043444736.0000, rpn_box: nan, rcnn_cls: 48455758502718176065170245156864.0000, rcnn_box nan\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter    7/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(26/486), time cost: 0.836596\n",
      "\t\t\tsiam_rpn_cls: 23.6278, siam_rpn_box: 5.0628\n",
      "\t\t\trpn_cls: 3223188376378873138456297472.0000, rpn_box: nan, rcnn_cls: 30631171328375120153542656.0000, rcnn_box nan\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter    8/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(14/498), time cost: 0.890093\n",
      "\t\t\tsiam_rpn_cls: 40.9077, siam_rpn_box: 1.4513\n",
      "\t\t\trpn_cls: 2334309729520337352700985344.0000, rpn_box: nan, rcnn_cls: nan, rcnn_box nan\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter    9/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(6/506), time cost: 0.860884\n",
      "\t\t\tsiam_rpn_cls: 0.6962, siam_rpn_box: 0.0016\n",
      "\t\t\trpn_cls: 5267353631018806386098176.0000, rpn_box: nan, rcnn_cls: nan, rcnn_box nan\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter   10/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(43/469), time cost: 0.862041\n",
      "\t\t\tsiam_rpn_cls: 0.6961, siam_rpn_box: 0.0084\n",
      "\t\t\trpn_cls: 0.6486, rpn_box: 0.0099, rcnn_cls: nan, rcnn_box nan\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter   11/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(32/480), time cost: 0.970377\n",
      "\t\t\tsiam_rpn_cls: 0.6931, siam_rpn_box: 0.0764\n",
      "\t\t\trpn_cls: 0.6505, rpn_box: 0.0697, rcnn_cls: nan, rcnn_box nan\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter   12/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(17/495), time cost: 0.952381\n",
      "\t\t\tsiam_rpn_cls: 0.6954, siam_rpn_box: 0.0273\n",
      "\t\t\trpn_cls: 0.6538, rpn_box: 0.0461, rcnn_cls: nan, rcnn_box nan\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter   13/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(99/413), time cost: 0.956556\n",
      "\t\t\tsiam_rpn_cls: 0.6968, siam_rpn_box: 0.0085\n",
      "\t\t\trpn_cls: 0.6517, rpn_box: 0.0070, rcnn_cls: nan, rcnn_box nan\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter   14/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(61/451), time cost: 0.927721\n",
      "\t\t\tsiam_rpn_cls: 0.6946, siam_rpn_box: 0.0308\n",
      "\t\t\trpn_cls: 0.6530, rpn_box: 0.0218, rcnn_cls: nan, rcnn_box nan\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter   15/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(31/481), time cost: 0.885440\n",
      "\t\t\tsiam_rpn_cls: 0.6935, siam_rpn_box: 0.0243\n",
      "\t\t\trpn_cls: 0.6555, rpn_box: 0.0377, rcnn_cls: nan, rcnn_box nan\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter   16/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(59/453), time cost: 0.889181\n",
      "\t\t\tsiam_rpn_cls: 0.6975, siam_rpn_box: 0.0072\n",
      "\t\t\trpn_cls: 0.6499, rpn_box: 0.0076, rcnn_cls: nan, rcnn_box nan\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter   17/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(59/453), time cost: 1.084087\n",
      "\t\t\tsiam_rpn_cls: 0.6937, siam_rpn_box: 0.0238\n",
      "\t\t\trpn_cls: 0.6522, rpn_box: 0.0371, rcnn_cls: nan, rcnn_box nan\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter   18/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(4/508), time cost: 0.859828\n",
      "\t\t\tsiam_rpn_cls: 0.6945, siam_rpn_box: 0.0097\n",
      "\t\t\trpn_cls: 0.6500, rpn_box: 0.0067, rcnn_cls: nan, rcnn_box nan\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter   19/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(70/442), time cost: 0.929689\n",
      "\t\t\tsiam_rpn_cls: 0.6929, siam_rpn_box: 0.0304\n",
      "\t\t\trpn_cls: 0.6516, rpn_box: 0.0233, rcnn_cls: nan, rcnn_box nan\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter   20/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(50/462), time cost: 0.829373\n",
      "\t\t\tsiam_rpn_cls: 0.6990, siam_rpn_box: 0.0097\n",
      "\t\t\trpn_cls: 0.6530, rpn_box: 0.0439, rcnn_cls: nan, rcnn_box nan\n",
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n",
      "[session 1][epoch  0][iter   21/  75] loss: 0.0000, lr: 1.00e-04\n",
      "\t\t\tfg/bg=(35/477), time cost: 0.882959\n",
      "\t\t\tsiam_rpn_cls: 0.6962, siam_rpn_box: 0.0143\n",
      "\t\t\trpn_cls: 0.6465, rpn_box: 0.0107, rcnn_cls: nan, rcnn_box nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lvye/anaconda3/envs/py27/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lvye/anaconda3/envs/py27/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lvye/anaconda3/envs/py27/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/lvye/anaconda3/envs/py27/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "    return recv()\n",
      "  File \"/home/lvye/anaconda3/envs/py27/lib/python2.7/site-packages/torch/multiprocessing/queue.py\", line 21, in recv\n",
      "    buf = self.recv_bytes()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: torch.Size([1])\n",
      "2: torch.Size([])\n",
      "3: torch.Size([1])\n",
      "4: torch.Size([])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-920c95a1bd04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"vgg16\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mclip_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lvye/anaconda3/envs/py27/lib/python2.7/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lvye/anaconda3/envs/py27/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_epoch = 10\n",
    "for epoch in range(max_epoch):\n",
    "    data_iter = iter(dataloader)\n",
    "    for step in range(iters_per_epoch):\n",
    "        data_1, data_2 = next(data_iter)\n",
    "\n",
    "        im_data_1.data.resize_(data_1[0].size()).copy_(data_1[0])\n",
    "        im_info_1.data.resize_(data_1[1].size()).copy_(data_1[1])\n",
    "        gt_boxes_1.data.resize_(data_1[2].size()).copy_(data_1[2])\n",
    "        num_boxes_1.data.resize_(data_1[3].size()).copy_(data_1[3])\n",
    "\n",
    "        im_data_2.data.resize_(data_2[0].size()).copy_(data_2[0])\n",
    "        im_info_2.data.resize_(data_2[1].size()).copy_(data_2[1])\n",
    "        gt_boxes_2.data.resize_(data_2[2].size()).copy_(data_2[2])\n",
    "        num_boxes_2.data.resize_(data_2[3].size()).copy_(data_2[3])\n",
    "        \n",
    "        #print(im_data.shape)\n",
    "        RCNN.zero_grad()\n",
    "        ##################################\n",
    "        #        Detection part          #\n",
    "        ##################################\n",
    "        # detection loss for image 1.\n",
    "        rois_1, cls_prob_1, bbox_pred_1, \\\n",
    "        rpn_loss_cls_1, rpn_loss_box_1, \\\n",
    "        RCNN_loss_cls_1, RCNN_loss_bbox_1, \\\n",
    "        rois_label_1 = RCNN(im_data_1, im_info_1, gt_boxes_1, num_boxes_1)\n",
    "\n",
    "        #c3_1, c4_1, c5_1 = RCNN.c_3, RCNN.c_4, RCNN.c_5\n",
    "        conv4_feat_1 = RCNN.Conv4_feat\n",
    "        rpn_rois_1 = RCNN.rpn_rois\n",
    "\n",
    "        # detection loss for image 2.\n",
    "        rois_2, cls_prob_2, bbox_pred_2, \\\n",
    "        rpn_loss_cls_2, rpn_loss_box_2, \\\n",
    "        RCNN_loss_cls_2, RCNN_loss_bbox_2, \\\n",
    "        rois_label_2 = RCNN(im_data_2, im_info_2, gt_boxes_2, num_boxes_2)\n",
    "\n",
    "        #c3_2, c4_2, c5_2 = RCNN.c_3, RCNN.c_4, RCNN.c_5\n",
    "        conv4_feat_2 = RCNN.Conv4_feat\n",
    "        rpn_rois_2 = RCNN.rpn_rois\n",
    "\n",
    "        ##################################\n",
    "        #        Tracking part           #\n",
    "        ##################################\n",
    "        # define tracking loss here.\n",
    "        tracking_losses_cls_ls = []\n",
    "        tracking_losses_box_ls = []\n",
    "        rtv_training_tuples = t_t_prop_layer(conv4_feat_1, conv4_feat_2, rpn_rois_1, gt_boxes_1, gt_boxes_2)\n",
    "        for tpl_id in range(len(rtv_training_tuples)):\n",
    "            target_feat, template_weights, target_gt_boxes = rtv_training_tuples[tpl_id]\n",
    "            input_v = (target_feat,\n",
    "                       im_info_2[tpl_id:tpl_id+1], \n",
    "                       template_weights,\n",
    "                       target_gt_boxes, \n",
    "                       1)\n",
    "            rois, rpn_loss_cls_siam, rpn_loss_box_siam = siameseRPN_layer(input_v)\n",
    "            tracking_losses_cls_ls.append(rpn_loss_cls_siam)\n",
    "            tracking_losses_box_ls.append(rpn_loss_box_siam)\n",
    "            \n",
    "        print('1:',rpn_loss_cls_1.shape)\n",
    "        print('2:',tracking_losses_cls_ls[0].shape)\n",
    "        \n",
    "        print('3:',rpn_loss_box_1.shape)\n",
    "        print('4:',tracking_losses_box_ls[0].shape)\n",
    "        \n",
    "        if len(tracking_losses_cls_ls)>0:\n",
    "            siamRPN_loss_cls = torch.mean(torch.stack(tracking_losses_cls_ls))\n",
    "        else:\n",
    "            siamRPN_loss_cls = rpn_loss_cls_2.new_zeros(1)\n",
    "        if len(tracking_losses_box_ls)>0:\n",
    "            siamRPN_loss_box = torch.mean(torch.stack(tracking_losses_box_ls))\n",
    "        else:\n",
    "            siamRPN_loss_box = rpn_loss_box_2.new_zeros(1)\n",
    "        \n",
    "        rpn_loss_cls = (rpn_loss_cls_1.mean()+rpn_loss_cls_2.mean())/2\n",
    "        rpn_loss_box = (rpn_loss_box_1.mean()+rpn_loss_box_2.mean())/2\n",
    "        RCNN_loss_cls = (RCNN_loss_cls_1.mean()+RCNN_loss_cls_2.mean())/2\n",
    "        RCNN_loss_bbox = (RCNN_loss_bbox_1.mean()+RCNN_loss_bbox_2.mean())/2\n",
    "        \n",
    "        loss=siamRPN_loss_cls+siamRPN_loss_box+rpn_loss_cls+rpn_loss_box+RCNN_loss_cls+RCNN_loss_bbox\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if args.net == \"vgg16\":\n",
    "            clip_gradient(RCNN, 10.)\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % args.disp_interval == 0:\n",
    "            end = time.time()\n",
    "            if step > 0:\n",
    "                loss_temp /= (args.disp_interval + 1)\n",
    "\n",
    "            if args.mGPUs:\n",
    "                loss_rpn_cls = rpn_loss_cls.mean().item()\n",
    "                loss_rpn_box = rpn_loss_box.mean().item()\n",
    "                loss_rcnn_cls = RCNN_loss_cls.mean().item()\n",
    "                loss_rcnn_box = RCNN_loss_bbox.mean().item()\n",
    "                loss_siamRPN_cls = siamRPN_loss_cls.mean().item()\n",
    "                loss_siamRPN_box = siamRPN_loss_box.mean().item()\n",
    "                fg_cnt_1 = torch.sum(rois_label_1.data.ne(0))\n",
    "                bg_cnt_1 = rois_label_1.data.numel() - fg_cnt_1\n",
    "                fg_cnt_2 = torch.sum(rois_label_2.data.ne(0))\n",
    "                bg_cnt_2 = rois_label_2.data.numel() - fg_cnt_2\n",
    "            else:\n",
    "                loss_rpn_cls = rpn_loss_cls.item()\n",
    "                loss_rpn_box = rpn_loss_box.item()\n",
    "                loss_rcnn_cls = RCNN_loss_cls.item()\n",
    "                loss_rcnn_box = RCNN_loss_bbox.item()\n",
    "                loss_siamRPN_cls = siamRPN_loss_cls.item()\n",
    "                loss_siamRPN_box = siamRPN_loss_box.item()\n",
    "                fg_cnt_1 = torch.sum(rois_label_1.data.ne(0))\n",
    "                bg_cnt_1 = rois_label_1.data.numel() - fg_cnt_1\n",
    "                fg_cnt_2 = torch.sum(rois_label_2.data.ne(0))\n",
    "                bg_cnt_2 = rois_label_2.data.numel() - fg_cnt_2\n",
    "            \n",
    "            fg_cnt = fg_cnt_1+fg_cnt_2\n",
    "            bg_cnt = bg_cnt_1+bg_cnt_2\n",
    "\n",
    "            print(\"[session %d][epoch %2d][iter %4d/%4d] loss: %.4f, lr: %.2e\" \\\n",
    "                                    % (args.session, epoch, step, iters_per_epoch, loss_temp, lr))\n",
    "            print(\"\\t\\t\\tfg/bg=(%d/%d), time cost: %f\" % (fg_cnt, bg_cnt, end-start))\n",
    "            print(\"\\t\\t\\tsiam_rpn_cls: %.4f, siam_rpn_box: %.4f\" \\\n",
    "                          % (loss_siamRPN_cls, loss_siamRPN_box))\n",
    "            print(\"\\t\\t\\trpn_cls: %.4f, rpn_box: %.4f, rcnn_cls: %.4f, rcnn_box %.4f\" \\\n",
    "                          % (loss_rpn_cls, loss_rpn_box, loss_rcnn_cls, loss_rcnn_box))\n",
    "\n",
    "            loss_temp = 0\n",
    "            start = time.time()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the training tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to change the kernel size in cfg.SIAMESE.TEMPLATE_SZ\n",
    "rtv_training_tuples = t_t_prop_layer(im_data_1, im_data_2, rpn_rois_1, gt_boxes_1, gt_boxes_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([1, 3, 600, 1067])\n",
      "torch.Size([4, 3, 5, 5])\n",
      "torch.Size([4, 1, 6])\n"
     ]
    }
   ],
   "source": [
    "print(len(rtv_training_tuples))\n",
    "target_feat, template_weights, target_gt_boxes = rtv_training_tuples[0]\n",
    "print(target_feat.shape)\n",
    "print(template_weights.shape)\n",
    "print(target_gt_boxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = im_data_1[0].cpu().numpy()\n",
    "plt.imshow(img.transpose(1,2,0))\n",
    "plt.show()\n",
    "\n",
    "img = im_data_2[0].cpu().numpy()\n",
    "plt.imshow(img.transpose(1,2,0))\n",
    "plt.show()\n",
    "\n",
    "img = np.zeros(im_data_2[0].cpu().numpy().transpose(1,2,0).shape,dtype = np.uint8)\n",
    "for it in range(target_gt_boxes.size(0)):\n",
    "    tgt = target_gt_boxes[it,0,:]#.int()\n",
    "    img = cv2.rectangle(img,(tgt[0],tgt[1]),(tgt[2],tgt[3]),(0,255,0),5)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 473.3333,  180.8333,  728.3333,  455.8333,    1.0000,    2.0000]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 473.3333,  180.8333,  728.3333,  455.8333,    1.0000,    2.0000]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 473.3333,  180.8333,  728.3333,  455.8333,    1.0000,    2.0000]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 473.3333,  180.8333,  728.3333,  455.8333,    1.0000,    2.0000]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAACHZJREFUeJzt3U+IXfUZxvHn6SQSqS0u6iJkQuNChBBohBIEuyhCYFpD7VJBV8JsKiTQIraLgqtuirjpJtig0KIIupChEAIN2IKN+WNsnURLkBYjwlBC0SBUYt4u5i5i69xzJvecOfc8+X5gYO7N7/7mJcx3zjn3DndcVQKQ6WtDDwCgPwQOBCNwIBiBA8EIHAhG4EAwAgeCETgQjMCBYNv62NQ2vx4H9Kyq3LSGIzgQjMCBYAQOBCNwIBiBA8EIHAhG4EAwAgeCETgQjMCBYAQOBCNwIBiBA8EIHAhG4EAwAgeCETgQrFXgtpdsv2/7ku2n+x4KQDfc9McHbS9I+rukg5IuSzot6dGqujDlMbxlE9Czrt6y6YCkS1X1QVV9LullSQ/POhyA/rUJfJekD2+4fXlyH4A519m7qtpelrTc1X4AZtcm8I8k7b7h9uLkvi+pqqOSjkpcgwPzos0p+mlJ99i+2/Ztkh6R9Hq/YwHoQuMRvKqu2X5S0nFJC5KOVdVq75MBmFnjy2Q3tSmn6EDv+MsmwC2OwIFgBA4EI3AgGIEDwQgcCEbgQDACB4IROBCMwIFgBA4EI3AgGIEDwQgcCEbgQDACB4IROBCMwIFgBA4EI3AgGIEDwQgcCEbgQDACB4IROBCMwIFgBA4EI3AgGIEDwQgcCEbgQDACB4IROBCMwIFgBA4Eawzc9jHba7bf3YqBAHSnzRH8BUlLPc8BoAeNgVfVG5KubMEsADrGNTgQbFtXG9lelrTc1X4AZueqal5k75G0UlX7Wm1qN28KYCZV5aY1nKIDwdq8TPaSpDcl3Wv7su0n+h8LQBdanaJvelNO0YHecYoO3OIIHAhG4EAwAgeCETgQjMCBYAQOBCNwIBiBA8EIHAhG4EAwAgeCETgQjMCBYAQOBCNwIBiBA8EIHAhG4EAwAgeCETgQjMCBYAQOBCNwIBiBA8EIHAhG4EAwAgeCETgQjMCBYAQOBCNwIBiBA8EIHAjWGLjt3bZP2r5ge9X24a0YDMDsXFXTF9g7Je2sqnO2vyHprKQfV9WFKY+ZvimAmVWVm9Y0HsGr6uOqOjf5/FNJFyXtmn08AH3b1DW47T2S7pN0qo9hAHRrW9uFtu+Q9KqkI1X1yVf8+7Kk5Q5nAzCjxmtwSbK9XdKKpONV9WyL9VyDAz1rcw3e5kk2S3pR0pWqOtLmCxM40L+uAv+epD9J+puk65O7f1FVf5jyGAIHetZJ4DeDwIH+dfIyGYDxInAgGIEDwQgcCEbgQDACB4IROBCMwIFgBA4EI3AgGIEDwQgcCEbgQDACB4IROBCMwIFgBA4EI3AgGIEDwQgcCEbgQDACB4IROBCMwIFgBA4EI3AgGIEDwQgcCEbgQDACB4IROBCMwIFgBA4EI3AgGIEDwRoDt73D9lu237G9avuZrRgMwOxcVdMX2Jb09aq6anu7pD9LOlxVf5nymOmbAphZVblpzbYWm5Skq5Ob2ycfBAyMQKtrcNsLts9LWpN0oqpO9TsWgC60Cryqvqiq/ZIWJR2wve9/19hetn3G9pmuhwRwcxqvwf/vAfYvJX1WVb+esoZTeKBnba7B2zyLfpftOyef3y7poKT3Zh8PQN8an2STtFPSi7YXtP4D4ZWqWul3LABd2PQpeqtNOUUHetfJKTqA8SJwIBiBA8EIHAhG4EAwAgeCETgQjMCBYAQOBCNwIBiBA8EIHAhG4EAwAgeCETgQjMCBYAQOBCNwIBiBA8EIHAhG4EAwAgeCETgQjMCBYAQOBCNwIBiBA8EIHAhG4EAwAgeCETgQjMCBYAQOBCNwIFjrwG0v2H7b9kqfAwHozmaO4IclXexrEADdaxW47UVJD0l6vt9xAHSp7RH8OUlPSbre4ywAOtYYuO1Dktaq6mzDumXbZ2yf6Ww6ADNxVU1fYP9K0uOSrknaIembkl6rqsemPGb6pgBmVlVuWtMY+JcW29+X9LOqOtSwjsCBnrUJnNfBgWCbOoK33pQjONA7juDALY7AgWAEDgQjcCAYgQPBCBwIRuBAMAIHghE4EIzAgWAEDgQjcCAYgQPBCBwIRuBAMAIHghE4EGxbT/v+S9I/O97zW5N9x2JM845pVmlc8/Y167fbLOrlLZv6YPtMVX136DnaGtO8Y5pVGte8Q8/KKToQjMCBYGMK/OjQA2zSmOYd06zSuOYddNbRXIMD2LwxHcEBbNIoAre9ZPt925dsPz30PNPYPmZ7zfa7Q8/SxPZu2ydtX7C9avvw0DNtxPYO22/Zfmcy6zNDz9SG7QXbb9teGeLrz33gthck/UbSDyTtlfSo7b3DTjXVC5KWhh6ipWuSflpVeyXdL+knc/x/+x9JD1bVdyTtl7Rk+/6BZ2rjsKSLQ33xuQ9c0gFJl6rqg6r6XNLLkh4eeKYNVdUbkq4MPUcbVfVxVZ2bfP6p1r8Rdw071VerdVcnN7dPPub6CSTbi5IekvT8UDOMIfBdkj684fZlzek34ZjZ3iPpPkmnhp1kY5PT3fOS1iSdqKq5nXXiOUlPSbo+1ABjCBw9s32HpFclHamqT4aeZyNV9UVV7Ze0KOmA7X1Dz7QR24ckrVXV2SHnGEPgH0nafcPtxcl96IDt7VqP+/dV9drQ87RRVf+WdFLz/VzHA5J+ZPsfWr+sfND277Z6iDEEflrSPbbvtn2bpEckvT7wTBFsW9JvJV2sqmeHnmca23fZvnPy+e2SDkp6b9ipNlZVP6+qxarao/Xv2T9W1WNbPcfcB15V1yQ9Kem41p8EeqWqVoedamO2X5L0pqR7bV+2/cTQM03xgKTHtX50OT/5+OHQQ21gp6STtv+q9R/6J6pqkJeexoTfZAOCzf0RHMDNI3AgGIEDwQgcCEbgQDACB4IROBCMwIFg/wVivOAIxuskrQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx in range(template_weights.size(0)):\n",
    "    print(target_gt_boxes[idx,:,:])\n",
    "    img = template_weights[idx,:,:,:].cpu().numpy()\n",
    "    plt.imshow(img.transpose(1,2,0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
